{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T00:56:34.774530Z",
     "start_time": "2024-11-02T00:32:51.487426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd656221b73747438bf490c99299de14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spdlq\\.conda\\envs\\level2-nlp-datacentric-nlp-04\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\spdlq\\.cache\\huggingface\\hub\\models--beomi--Llama-3-Open-Ko-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d79409f997794a6586eb74fad014fe67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d63dd3fc9406463dbe2ba75fc56647eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc72b4c82ddc4041bce00b95754138a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30c69ec0fcb448bea71156c67e69de29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d52a0774ecee4886862a48e37562bfbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b872cdb6fdee417aa70d4f7ba4bde2ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bb977b74f4b4491a77022468ce040f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93ee088ab6114ff5819244e7c3adc3fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "721078251e9649508b5da1df882fb5aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78a00991e2d94f889ab2bbd3b36a5856"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41dfec344c1e4c2596783ff9329dd4c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f70d21a31e3b45e6b9b82d3c780467e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90580c78187b4209934015d704f5b309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T01:09:33.276559Z",
     "start_time": "2024-11-02T01:08:40.847805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 텍스트 생성 예제\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs.input_ids, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hello, how are you? I'm AAA, the sales team leader of BBB.\n",
      "Hello, how are you? I'm AAA, the sales team leader of BBB.\n",
      "안녕하세요, 잘 지내셨나요? BBB사의 영업팀장\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LangChain에서 Hugging Face 모델을 연결하기 위해, 먼저 Transformers 라이브러리로 모델을 로드한 뒤 pipeline 객체를 생성합니다."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T01:29:19.226402Z",
     "start_time": "2024-11-02T01:29:19.220403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# 파이프라인 생성\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1000,  # 생성할 최대 토큰 길이\n",
    "    temperature=0.5,\n",
    "    do_sample=True,  # 샘플링을 활성화하여 temperature를 사용  # 텍스트 생성의 다양성을 제어하는 하이퍼파라미터\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# LangChain의 HuggingFacePipeline 래퍼 사용\n",
    "llama_chain = HuggingFacePipeline(pipeline=hf_pipeline)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-02T01:29:19.883660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "# 프롬프트를 입력으로 모델에서 텍스트 생성\n",
    "prompt = \"\"\"\n",
    "다음 텍스트에서 아스키코드 기반으로 노이즈가 들어간 단어들을 자연스럽게 고쳐줘:\n",
    "\"pI美대선I앞두고 R2fr단 발] $비해 감시 강화\"\n",
    "아스키코드기반 문자 대체를 통해 노이즈가 들어간 데이터야. 다시 디노이징 해줘\"\n",
    "\"\"\"\n",
    "\n",
    "# 시간 측정 시작\n",
    "start_time = time.time()\n",
    "# 모델에서 텍스트 생성\n",
    "result = llama_chain.invoke(prompt)\n",
    "# 시간 측정 종료\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"입력 프롬프트 : {prompt}\")\n",
    "# 결과 출력\n",
    "print(f\"출력 프롬프트 : {result}\")\n",
    "\n",
    "# 걸린 시간 출력\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"걸린 시간: {elapsed_time:.2f}초\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
