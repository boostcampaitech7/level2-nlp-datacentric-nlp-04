{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T03:33:37.253694Z",
     "start_time": "2024-11-05T03:33:35.016239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "import torch\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-05T03:48:03.378824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "import transformers\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Define the chat messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Convert messages to prompt format using the pipeline tokenizer\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Define the end-of-sequence token IDs for the model\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Print the generated text without the prompt prefix\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "568d45e0bcf74b5fb616c627ebc1fc39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LangChain에서 Hugging Face 모델을 연결하기 위해, 먼저 Transformers 라이브러리로 모델을 로드한 뒤 pipeline 객체를 생성합니다."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T03:40:07.819390Z",
     "start_time": "2024-11-05T03:40:04.895199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move model to CUDA device 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set pad_token_id if not already set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Prepare the input text\n",
    "input_text = \"너는 누구니?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)  # Generate inputs\n",
    "\n",
    "# Ensure inputs are on the same device as the model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate text with attention mask\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Provide attention mask\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Decode and print generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: 너는 누구니? / 너는 누구니? / 너는 누구니? (너는 누구니?) / 너는 누구니? (너는 누구니?) (너는 누구니?)\n",
      "Title / Romaji: Neoneun\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T03:42:42.590493Z",
     "start_time": "2024-11-05T03:42:38.392795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move model to CUDA device 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: 너는 누구니? (Who are you?)\n",
      "너는 누구니?\n",
      "Korean: 너는 누구니\n",
      "Hangul: Neoneun Nuguni\n",
      "Director: Kim Sung-Su, Kang Hyung-Chul\n",
      "Writer: Yoo Seong-Ho\n",
      "Networks: SBS, iTV\n",
      "A series about the 20\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:25:48.254871Z",
     "start_time": "2024-11-03T09:25:43.903822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, professional assistant named 민봇. Introduce yourself first, and answer the questions. answer me in Korean no matter what. \"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llama_chain\n",
    "chain.invoke({\"input\": \"LLM이 뭐야?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful, professional assistant named 민봇. Introduce yourself first, and answer the questions. answer me in Korean no matter what. \\nHuman: LLM이 뭐야? AI야? AI야? AI야'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T09:24:39.665840Z",
     "start_time": "2024-11-03T09:24:35.153806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llama_chain| StrOutputParser()\n",
    "\n",
    "chain.invoke({\"input\": \"대한민국 수도가 어디야?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful, professional assistant named 민봇. Introduce yourself first, and answer the questions. answer me in Korean no matter what. \\nHuman: 대한민국 수도가 어디야? \\nSystem: 대한민국 수도는 서울특별시입니다'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T07:48:38.617030Z",
     "start_time": "2024-11-03T07:48:38.611034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T07:48:45.301760Z",
     "start_time": "2024-11-03T07:48:45.290761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 사용할 Few-shot예제를 정의합니다.\n",
    "# 여기서도 마찬가지로 뉴스 제목 생성에 관해 진행하겠습니다.\n",
    "examples = [\n",
    "    {\n",
    "        \"domain\": \"경제\",\n",
    "        \"answer\": \"ESG 투자 확산, 기업들이 새로운 경영 패러다임을 맞이하다\",\n",
    "    },\n",
    "    {\n",
    "        \"domain\": \"정치\",\n",
    "        \"answer\": \"정당 간 연합 논의 본격화, 정치 지형에 미칠 영향은\",\n",
    "    },\n",
    "    {\n",
    "        \"domain\": \"과학\",\n",
    "        \"answer\": \"유전자 편집 기술의 미래: CRISPR의 혁신과 윤리적 논의\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# 입력할 도메인, 그리고 답변을 PromptTemplate에 입력합니다.\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Domain:\\n{domain}\\nAnswer:\\n{answer}\"\n",
    ")\n",
    "print(example_prompt.format(**examples[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain:\n",
      "경제\n",
      "Answer:\n",
      "ESG 투자 확산, 기업들이 새로운 경영 패러다임을 맞이하다\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:06.843872Z",
     "start_time": "2024-11-03T07:49:06.832868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### TODO\n",
    "# example, example_prompt, input_variables를 활용해주세요\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Domain:\\n{domain}\\nAnswer:\",\n",
    "    input_variables=[\"domain\"],\n",
    ")\n",
    "###\n",
    "\n",
    "domain = \"환경\"\n",
    "final_prompt = prompt.format(domain=domain)\n",
    "print(final_prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain:\n",
      "경제\n",
      "Answer:\n",
      "ESG 투자 확산, 기업들이 새로운 경영 패러다임을 맞이하다\n",
      "\n",
      "Domain:\n",
      "정치\n",
      "Answer:\n",
      "정당 간 연합 논의 본격화, 정치 지형에 미칠 영향은\n",
      "\n",
      "Domain:\n",
      "과학\n",
      "Answer:\n",
      "유전자 편집 기술의 미래: CRISPR의 혁신과 윤리적 논의\n",
      "\n",
      "Domain:\n",
      "환경\n",
      "Answer:\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T07:51:30.548468Z",
     "start_time": "2024-11-03T07:50:38.519519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 결과 출력\n",
    "answer = llama_chain.invoke(final_prompt)\n",
    "print(answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain:\n",
      "경제\n",
      "Answer:\n",
      "ESG 투자 확산, 기업들이 새로운 경영 패러다임을 맞이하다\n",
      "\n",
      "Domain:\n",
      "정치\n",
      "Answer:\n",
      "정당 간 연합 논의 본격화, 정치 지형에 미칠 영향은\n",
      "\n",
      "Domain:\n",
      "과학\n",
      "Answer:\n",
      "유전자 편집 기술의 미래: CRISPR의 혁신과 윤리적 논의\n",
      "\n",
      "Domain:\n",
      "환경\n",
      "Answer:이번에 새로 나온 신제품입니다.\n",
      "Dies ist ein neues Produkt.\n",
      "국내에서 코로나19 백신 접종이 시작된 지 100일을 맞았다. 2일 코로나19 예방접종대응추진단에 따르면 이날 0시 기준 누적 접종자는 1,000만 2,000명이다. 전 국민의 19.5%가 1차 접종을 마쳤고, 2차 접종\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
